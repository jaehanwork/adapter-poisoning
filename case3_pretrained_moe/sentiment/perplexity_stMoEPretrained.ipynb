{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bf91ee-eabc-4678-8dc0-f544592aecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser('~/.env'), verbose=True)\n",
    "\n",
    "data_dir = os.getenv('DATA_IGN_DIR')\n",
    "pretrained_model_dir = os.getenv('PRETRAINED_MODEL_DIR')\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7048f5-5b82-4fa6-8d3a-5ccde0de0180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from st_moe_distilgpt2_perplexity import STMoE_DistilGPT2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_count = torch.cuda.device_count()\n",
    "print(device, device_count)\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "\n",
    "    'rotten_tomatoes': (\"text\", None),\n",
    "    'imdb': (\"text\", None),\n",
    "    'yelp_polarity': (\"text\", None),\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "eval_data_dict = {'imdb': 'test', 'yelp_polarity': 'test'}\n",
    "\n",
    "is_glue = {\"cola\": True,\n",
    "            \"mnli\": True,\n",
    "            \"mrpc\": True,\n",
    "            \"qnli\": True,\n",
    "             \"qqp\": True,\n",
    "             \"rte\": True,\n",
    "            \"sst2\": True,\n",
    "            \"stsb\": True,\n",
    "            \"wnli\": True,}\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd0aa2f-2ac7-4004-a97d-3609af341401",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'checkpoint_4.pt'\n",
    "checkpoint_path = os.path.join(pretrained_model_dir, checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db54b13c-8b24-40a2-b2ee-92d3146ba44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50258\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('distilGPT2')\n",
    "model = STMoE_DistilGPT2()\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "print(model.net.transformer.config.vocab_size)\n",
    "\n",
    "model.net.config.pad_token_id = None\n",
    "model.net.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model.net.transformer.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9900872f-285e-45e0-b0bf-38428e757a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3defb9d-2853-4c42-a59d-c2658356b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe749be-8eb9-4e0d-95cc-80eff68f75de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 280/281 [00:06<00:00, 40.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(112195.5234, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "max_length = model.net.config.n_positions\n",
    "stride = 1024\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # 마지막 루프의 스트라이드 값과 다를 수 있음\n",
    "    \n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "\n",
    "        lm_logits = outputs[0]\n",
    "        \n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = target_ids[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    nlls.append(loss)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb99d77-726e-4a57-b39a-081f528e3ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case3",
   "language": "python",
   "name": "case3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
