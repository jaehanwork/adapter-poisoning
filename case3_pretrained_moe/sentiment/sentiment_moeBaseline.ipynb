{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d824c318-93d3-410a-8a0b-253b4ea85c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser('~/.env'), verbose=True)\n",
    "\n",
    "data_dir = os.getenv('DATA_IGN_DIR')\n",
    "adapter_lib_path = os.getenv('ADAPTER_LIB_PATH')\n",
    "\n",
    "sys.path.insert(0, adapter_lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7048f5-5b82-4fa6-8d3a-5ccde0de0180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    get_scheduler,\n",
    "    PfeifferConfig\n",
    ")\n",
    "from transformers.adapters import AdapterArguments, AdapterTrainer, AdapterConfigBase, AutoAdapterModel, setup_adapter_training\n",
    "from transformers import AdapterConfig, EvalPrediction, TextClassificationPipeline\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from pdb import set_trace\n",
    "import transformers.adapters.composition as ac\n",
    "\n",
    "from transformers.adapters.heads import ClassificationHead\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "from datasets import concatenate_datasets, ClassLabel, Value\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_count = torch.cuda.device_count()\n",
    "print(device, device_count)\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "\n",
    "    'rotten_tomatoes': (\"text\", None),\n",
    "    'imdb': (\"text\", None),\n",
    "    'yelp_polarity': (\"text\", None),\n",
    "    \n",
    "}\n",
    "\n",
    "# adapter_info = {'cola': {'load_adapter': 'lingaccept/cola@ukp', 'adapter_config': 'pfeiffer'},\n",
    "#                 # 'mnli'\n",
    "#                 'mrpc': {'load_adapter': 'sts/mrpc@ukp',        'adapter_config': 'pfeiffer'},\n",
    "#                 'qnli': {'load_adapter': 'nli/qnli@ukp',        'adapter_config': 'pfeiffer'},\n",
    "#                 'qqp' : {'load_adapter': 'sts/qqp@ukp',         'adapter_config': 'pfeiffer'},\n",
    "#                 'rte' : {'load_adapter': 'nli/rte@ukp',         'adapter_config': 'pfeiffer'},\n",
    "#                 'sst2': {'load_adapter': 'sentiment/sst-2@ukp', 'adapter_config': 'pfeiffer'},\n",
    "#                 'stsb': {'load_adapter': 'sts/sts-b@ukp',       'adapter_config': 'pfeiffer'},\n",
    "                \n",
    "#                 'rotten_tomatoes': {'load_adapter': 'AdapterHub/bert-base-uncased-pf-rotten_tomatoes', 'adapter_config': 'pfeiffer'},\n",
    "#                 'imdb': {'load_adapter': 'AdapterHub/bert-base-uncased-pf-imdb', 'adapter_config': 'pfeiffer'},\n",
    "#                 'yelp_polarity': {'load_adapter': 'AdapterHub/bert-base-uncased-pf-yelp_polarity', 'adapter_config': 'pfeiffer'},\n",
    "#                }\n",
    "\n",
    "adapter_info = {\n",
    "                'bert-base-uncased':\n",
    "                    {\n",
    "                        'imdb': 'AdapterHub/roberta-base-pf-imdb',\n",
    "                        'rotten_tomatoes': 'AdapterHub/roberta-base-pf-rotten_tomatoes',\n",
    "                        'sst2': 'AdapterHub/roberta-base-pf-sst2',\n",
    "                        'yelp_polarity': 'AdapterHub/roberta-base-pf-yelp_polarity'\n",
    "                    },\n",
    "                'roberta-base':\n",
    "                    {      \n",
    "                        'imdb': 'AdapterHub/roberta-base-pf-imdb',\n",
    "                        'rotten_tomatoes': 'AdapterHub/roberta-base-pf-rotten_tomatoes',\n",
    "                        'sst2': 'AdapterHub/roberta-base-pf-sst2',\n",
    "                        'yelp_polarity': 'AdapterHub/roberta-base-pf-yelp_polarity'\n",
    "                    }\n",
    "               }\n",
    "\n",
    "eval_data_dict = {'imdb': 'test', 'yelp_polarity': 'test'}\n",
    "\n",
    "is_glue = {\"cola\": True,\n",
    "            \"mnli\": True,\n",
    "            \"mrpc\": True,\n",
    "            \"qnli\": True,\n",
    "             \"qqp\": True,\n",
    "             \"rte\": True,\n",
    "            \"sst2\": True,\n",
    "            \"stsb\": True,\n",
    "            \"wnli\": True,}\n",
    "\n",
    "metric_dict = {'rotten_tomatoes': 'sst2', 'imdb': 'sst2', 'yelp_polarity': 'sst2'}\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd0aa2f-2ac7-4004-a97d-3609af341401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(sys.argv) - 1 != 2:\n",
    "#     print('Argument error')\n",
    "#     exit(1)\n",
    "\n",
    "# _, arg1, arg2 = sys.argv\n",
    "\n",
    "# task_name_1 = arg1\n",
    "# adapter_count = int(arg2)\n",
    "\n",
    "task_name_1= 'rotten_tomatoes'\n",
    "adapter_count = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e77c17-5a2f-4f95-baff-678167dab5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "task_name_str = f'moe_sentiment_{task_name_1}_{adapter_count}E'\n",
    "model_name_or_path = 'roberta-base'\n",
    "pad_to_max_length = True\n",
    "max_seq_length = 128\n",
    "output_dir = os.path.join(data_dir, f'tmp_case3_sentiment_moeBaseline/{task_name_str}_{current_time}')\n",
    "\n",
    "\n",
    "adapter_config_default = 'pfeiffer'\n",
    "\n",
    "adapter_k = 2\n",
    "noisy_gating = True\n",
    "gating_layer = None\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "train_test_ratio = 0.2\n",
    "random_seed = 0\n",
    "\n",
    "set_seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc47007-8d23-4116-85aa-0486e82f45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")\n",
    "\n",
    "def load_dataset_with_glue(task_name):\n",
    "    if task_name == 'scitail':\n",
    "        return load_dataset(task_name, 'tsv_format')\n",
    "    elif task_name in is_glue:\n",
    "        return load_dataset('glue', task_name)\n",
    "    else:\n",
    "        return load_dataset(task_name)\n",
    "\n",
    "def get_eval_dataset(dataset, task_name):\n",
    "    if task_name == 'snli' or task_name == 'imdb' or task_name == 'yelp_polarity':\n",
    "        return dataset['test']\n",
    "    elif task_name == 'mnli':\n",
    "        return dataset['validation_matched']\n",
    "    else:\n",
    "        return dataset['validation']\n",
    "\n",
    "def get_data(task_name, raw_datasets):\n",
    "    sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "\n",
    "    if pad_to_max_length:\n",
    "        padding = \"max_length\"\n",
    "\n",
    "    def preprocess_function(examples):    \n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    \n",
    "        # Map labels to IDs (not necessary for GLUE tasks)\n",
    "        # if label_to_id is not None and \"label\" in examples:\n",
    "            # result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "        result[\"label\"] = [(l if l != -1 else -1) for l in examples[\"label\"]]\n",
    "        return result\n",
    "        \n",
    "    raw_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a7e850-54e7-40fe-a384-18e3402519fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset_with_glue(task_name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96974c5-73b3-4bb2-b590-b04bf7f4ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_data(task_name_1, raw_datasets)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "_train_dataset = dataset['train'].train_test_split(test_size=train_test_ratio, shuffle=True, seed=random_seed)\n",
    "\n",
    "train_dataset = _train_dataset['train']\n",
    "valid_dataset = _train_dataset['test']\n",
    "\n",
    "eval_dataset = get_eval_dataset(dataset, task_name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78bf8ac-4ad5-4d6e-a683-a2bd0d8adfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 6824\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e56b49-5d25-4e99-adf8-9acf122ee7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1706\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47e6c00c-6be5-4177-9f7c-bce6bf216f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1066\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db54b13c-8b24-40a2-b2ee-92d3146ba44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoAdapterModel.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "model.freeze_model(True)\n",
    "\n",
    "loaded_adapters = []\n",
    "for i in range(adapter_count):\n",
    "    adapter_name = f'expert_{i}'\n",
    "    model.add_adapter(adapter_name, config=adapter_config_default)\n",
    "    loaded_adapters.append(adapter_name)\n",
    "\n",
    "model.active_adapters = ac.Parallel(*loaded_adapters, mode='gating_token')\n",
    "\n",
    "model.init_gating_network(task_name_str, adapter_k, noisy_gating, gating_layer)\n",
    "\n",
    "model.add_classification_head(task_name_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c049c1bb-d070-4c76-93e2-89ec0a29045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "expert_0                 bottleneck          894,528       0.716       1       1\n",
      "expert_1                 bottleneck          894,528       0.716       1       1\n",
      "expert_2                 bottleneck          894,528       0.716       1       1\n",
      "expert_3                 bottleneck          894,528       0.716       1       1\n",
      "expert_4                 bottleneck          894,528       0.716       1       1\n",
      "expert_5                 bottleneck          894,528       0.716       1       1\n",
      "expert_6                 bottleneck          894,528       0.716       1       1\n",
      "expert_7                 bottleneck          894,528       0.716       1       1\n",
      "expert_8                 bottleneck          894,528       0.716       1       1\n",
      "expert_9                 bottleneck          894,528       0.716       1       1\n",
      "expert_10                bottleneck          894,528       0.716       1       1\n",
      "expert_11                bottleneck          894,528       0.716       1       1\n",
      "expert_12                bottleneck          894,528       0.716       1       1\n",
      "expert_13                bottleneck          894,528       0.716       1       1\n",
      "expert_14                bottleneck          894,528       0.716       1       1\n",
      "expert_15                bottleneck          894,528       0.716       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               124,940,544     100.000               1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7d46258-b57e-4aa1-8065-5241070b65e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moe_sentiment_rotten_tomatoes_16E'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.active_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524b6403-2b43-4fb1-a6f6-ba6b10a9c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    if 'heads' in k or 'gating' in k or 'adapter' in k:\n",
    "        v.requires_grad = True\n",
    "    else:\n",
    "        v.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a59c13-148a-40ff-8681-b396c51f09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,199,490 / 139,845,122\n"
     ]
    }
   ],
   "source": [
    "total_params = format(sum(p.numel() for p in model.parameters()), ',')\n",
    "total_params_train = format(sum(p.numel() for p in model.parameters() if p.requires_grad), ',')\n",
    "print(f'{total_params_train} / {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8924ee-3913-4283-a18b-9b0c96430039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in model.named_parameters():\n",
    "#     if v.requires_grad:\n",
    "#         print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a24f25e-fbd2-42dd-9ed6-dd3b9e5c57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 32\n",
    "per_device_eval_batch_size = 512\n",
    "weight_decay = 0.0\n",
    "learning_rate = 1e-3\n",
    "num_train_epochs = 10\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.1\n",
    "patience = 4\n",
    "alpha_info = 0.5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_batch_size_train = per_device_train_batch_size * device_count\n",
    "total_batch_size_eval = per_device_eval_batch_size * device_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a007ecf3-1e37-4afd-b6bd-4a0864943868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "def accuracy_topk_score(y_true, y_pred, k=1):\n",
    "    score = []\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        score.append(1 if y_t in y_p[:k] else 0)\n",
    "\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6310bfb1-440c-4baf-8c4c-108c1ad83822",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to='all',\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_dir=\"./logs\",\n",
    "    seed=random_seed,\n",
    "    data_seed=random_seed,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    # evaluation_strategy='steps',\n",
    "    # logging_strategy='steps',\n",
    "    # save_strategy='steps',\n",
    "    # eval_steps=2000,\n",
    "    # logging_steps=2000,\n",
    "    # save_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'loss'\n",
    ")\n",
    "\n",
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "def get_gating_data(model):\n",
    "    gate_scores = []\n",
    "    gate_losses = []\n",
    "    for i, encoder_layer in enumerate(model.base_model.encoder.layer):\n",
    "        gate_score = encoder_layer.output.gating_data.pop('gate_score')\n",
    "        gate_loss = encoder_layer.output.gating_data.pop('gate_loss')\n",
    "\n",
    "        gate_scores.append(gate_score)\n",
    "        \n",
    "        if gating_layer and i not in gating_layer:\n",
    "            continue\n",
    "        \n",
    "        gate_losses.append(gate_loss)\n",
    "\n",
    "\n",
    "    return gate_scores, torch.stack(gate_losses, 0).mean(0)\n",
    "\n",
    "def loss_gating(logits, gate_loss, labels):\n",
    "    loss_cls = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "    total_loss = ((1 - alpha_info) * loss_cls) + (alpha_info * gate_loss)\n",
    "    return total_loss, loss_cls, gate_loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        labels = inputs.pop('labels')\n",
    "\n",
    "        # Compute model outputs\n",
    "        outputs = model(**inputs)\n",
    "        gate_scores, gate_loss = get_gating_data(model)\n",
    "\n",
    "        logits = outputs[0].logits\n",
    "        \n",
    "        loss, _, _ = loss_gating(logits, gate_loss, labels)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader,\n",
    "        description: str,\n",
    "        prediction_loss_only: Optional[bool] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ):\n",
    "        # This is a simple modification. For more custom behavior, \n",
    "        # you might want to start from the original code in Trainer's evaluation_loop.\n",
    "        \n",
    "        # Initialize metrics, etc.\n",
    "        self.model.eval()\n",
    "        total_eval_loss = 0.0\n",
    "        total_eval_loss_cls = 0.0\n",
    "        total_eval_loss_gate = 0.0\n",
    "        total_preds = []\n",
    "        total_logits = []\n",
    "        total_labels = []\n",
    "        total_eval_metrics = {}\n",
    "\n",
    "        adapter_freq = np.array([[0] * adapter_count] * len(model.base_model.encoder.layer))\n",
    "        \n",
    "        for step, inputs in enumerate(dataloader):\n",
    "            labels = inputs.pop('labels').to(self.args.device)\n",
    "            \n",
    "            # Move inputs to appropriate device\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.args.device)\n",
    "            \n",
    "            # Forward pass and compute loss and metrics\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                gate_scores, gate_loss = get_gating_data(model)\n",
    "\n",
    "                logits = outputs[0].logits\n",
    "\n",
    "            loss, loss_cls, loss_gate = loss_gating(logits, gate_loss, labels)\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "            total_eval_loss_cls += loss_cls.item()\n",
    "            total_eval_loss_gate += loss_gate.item()\n",
    "\n",
    "            for i, gate_scores_layer in enumerate(gate_scores):\n",
    "                for gate_scores_batch in gate_scores_layer:\n",
    "                    top_scores_batch, top_indices_batch = gate_scores_batch.topk(adapter_k, dim=1)\n",
    "                    for top_indices in top_indices_batch:\n",
    "                        for top_index in top_indices:\n",
    "                            adapter_freq[i][top_index] += 1\n",
    "\n",
    "            total_logits.extend(logits.detach().cpu().numpy())\n",
    "            total_preds.extend(logits.argmax(dim=-1))\n",
    "            total_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        average_eval_loss = total_eval_loss / len(dataloader)\n",
    "        average_eval_loss_cls = total_eval_loss_cls / len(dataloader)\n",
    "        average_eval_loss_gate = total_eval_loss_gate / len(dataloader)\n",
    "        \n",
    "        eval_pred = EvalPrediction(predictions=total_logits, label_ids=total_labels)\n",
    "        \n",
    "        metrics = self.compute_metrics(eval_pred)\n",
    "\n",
    "        num_eval_samples = len(dataloader.dataset)\n",
    "\n",
    "        all_adapter_freq = np.round(adapter_freq / num_eval_samples, decimals=4)\n",
    "        avg_adapter_freq = np.around(np.mean(adapter_freq, axis=0) / num_eval_samples, decimals=4)\n",
    "        \n",
    "        if gating_layer and len(gating_layer) == 1:\n",
    "            freq_all = None\n",
    "        else:\n",
    "            freq_all = [list(o) for o in all_adapter_freq]\n",
    "            \n",
    "        total_eval_metrics = {f'{metric_key_prefix}_loss': average_eval_loss,\n",
    "                              f'{metric_key_prefix}_loss_cls': average_eval_loss_cls,\n",
    "                              f'{metric_key_prefix}_loss_gate': average_eval_loss_gate,\n",
    "                              f'{metric_key_prefix}_accuracy': metrics['accuracy'],\n",
    "                              f'{metric_key_prefix}_gate_freq_avg': list(avg_adapter_freq),\n",
    "                              f'{metric_key_prefix}_gate_freq_all': freq_all,\n",
    "                             }\n",
    "\n",
    "        # return total_eval_loss, total_eval_metrics\n",
    "        return EvalLoopOutput(predictions=total_preds, \n",
    "                              label_ids=total_labels, \n",
    "                              metrics=total_eval_metrics, \n",
    "                              num_samples=num_eval_samples)\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df5dcbc-4b57-4d88-a602-1a59b5a3001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehan/research/adapter/adapter-poisoning/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6824\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2140\n",
      "  Number of trainable parameters = 15199490\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1712' max='2140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1712/2140 40:44 < 10:11, 0.70 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss Cls</th>\n",
       "      <th>Loss Gate</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Gate Freq Avg</th>\n",
       "      <th>Gate Freq All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.452187</td>\n",
       "      <td>0.793406</td>\n",
       "      <td>0.110969</td>\n",
       "      <td>0.810082</td>\n",
       "      <td>[23.9297, 13.7951, 9.2984, 11.6721, 11.4894, 19.1574, 20.97, 17.5376, 16.7266, 15.8474, 16.8836, 21.9895, 18.5515, 13.2515, 17.7794, 7.121]</td>\n",
       "      <td>[[0.6934, 25.8136, 4.939, 83.8986, 13.4988, 0.0, 0.7456, 0.2808, 3.7948, 89.7397, 5.1161, 2.4719, 5.9461, 19.0188, 0.027, 0.0158], [90.3083, 93.9789, 0.3224, 0.8992, 0.3476, 0.1032, 0.0035, 28.9027, 0.5651, 1.1606, 12.7673, 14.5563, 2.3775, 0.1483, 1.6172, 7.942], [102.2755, 0.0311, 1.2732, 2.4601, 0.0064, 0.7169, 0.0, 0.0346, 10.4256, 21.0516, 81.7544, 0.0545, 10.1243, 10.337, 0.7339, 14.721], [0.0, 0.0018, 0.0064, 7.4631, 0.0, 65.6184, 6.6172, 10.2655, 8.0703, 12.5258, 0.0903, 11.4742, 1.1917, 60.1577, 72.4332, 0.0844], [0.1325, 9.9725, 3.5762, 2.2884, 24.49, 18.871, 1.4402, 11.9097, 0.1712, 5.6565, 10.7649, 112.8593, 7.0803, 3.4543, 34.9959, 8.337], [7.221, 24.5012, 18.4994, 3.558, 0.1002, 77.272, 26.9074, 0.7052, 0.0006, 5.8054, 0.9678, 8.4285, 55.4056, 7.2198, 7.0035, 12.4045], [5.0457, 0.0, 53.6753, 9.9988, 0.32, 9.422, 13.2696, 63.9596, 45.3066, 0.0305, 4.8916, 13.4637, 0.0012, 5.2016, 31.4138, 0.0], [0.0569, 8.3001, 0.8347, 0.3599, 29.7526, 16.374, 20.6213, 50.745, 0.1366, 0.0, 26.1383, 40.541, 42.3986, 2.9426, 0.136, 16.6624], [20.3828, 0.0006, 0.2075, 11.5563, 42.2937, 15.6102, 1.3271, 1.6166, 53.7098, 3.9965, 42.8283, 0.0604, 12.313, 0.1395, 46.2444, 3.7134], [0.0, 1.8681, 4.7831, 5.1413, 7.9232, 23.6125, 72.6483, 1.9555, 0.0182, 48.4625, 17.2585, 35.5809, 21.1841, 0.0, 0.0041, 15.5598], [16.0229, 1.0733, 23.4631, 12.4414, 18.4912, 2.289, 46.296, 39.9004, 3.6084, 0.0, 0.0217, 24.3834, 5.6231, 39.3535, 18.7433, 4.2896], [45.017, 0.0, 0.0, 0.0, 0.6483, 0.0, 61.7632, 0.1758, 74.9115, 1.7397, 0.0047, 0.0, 58.973, 11.0445, 0.0, 1.7222]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.207948</td>\n",
       "      <td>0.312325</td>\n",
       "      <td>0.103570</td>\n",
       "      <td>0.871630</td>\n",
       "      <td>[8.4195, 9.5398, 18.2963, 15.9447, 16.9168, 16.2247, 20.1295, 16.7506, 19.6154, 11.5378, 26.8729, 8.6438, 14.7053, 24.5061, 12.8042, 15.0927]</td>\n",
       "      <td>[[0.0147, 24.2691, 82.0076, 3.908, 8.8224, 0.0, 40.8453, 33.0574, 18.51, 0.0, 0.0123, 0.323, 4.0358, 0.4004, 5.7052, 34.0891], [0.0604, 2.0199, 0.1841, 17.4543, 11.1483, 1.6442, 0.7005, 25.7327, 67.371, 17.5293, 61.0281, 0.5012, 0.5574, 44.0264, 3.0059, 3.0363], [8.0557, 1.0076, 0.2122, 5.1372, 42.5516, 3.517, 12.5721, 10.1348, 28.2767, 0.0123, 8.0, 1.078, 87.1817, 9.2743, 28.3277, 10.6612], [0.0012, 0.0938, 0.6213, 0.0, 0.0, 30.9496, 1.5698, 7.8341, 15.8013, 12.3353, 73.6489, 2.3177, 5.6571, 104.3927, 0.1342, 0.643], [0.4607, 16.0727, 1.6284, 56.7034, 24.296, 66.9777, 10.9338, 4.9513, 0.3828, 10.17, 1.8118, 3.8136, 0.6049, 16.6254, 26.0615, 14.5059], [6.1934, 25.7597, 23.4226, 23.2057, 2.7737, 15.1964, 33.4132, 0.0, 0.0, 8.5797, 3.9871, 41.561, 0.1366, 15.3236, 38.1676, 18.2796], [3.0826, 0.9273, 39.7579, 8.1043, 8.0305, 29.7198, 32.5334, 15.2362, 32.1055, 2.364, 63.3687, 1.4179, 3.711, 9.8066, 4.2028, 1.6313], [12.9941, 3.5029, 4.5305, 2.2409, 19.7421, 22.1952, 19.8705, 9.4543, 3.4068, 34.1032, 38.9209, 11.2907, 2.0657, 11.262, 0.1096, 60.3107], [10.7773, 10.4179, 2.6225, 36.1659, 14.0674, 0.786, 15.2315, 50.7884, 0.9683, 17.857, 3.4607, 8.4478, 4.493, 19.7474, 37.9678, 22.2011], [29.8892, 18.578, 28.9865, 19.7081, 38.6079, 3.5533, 39.5358, 1.6008, 10.4144, 0.3546, 18.7069, 27.2737, 6.2784, 2.459, 4.1717, 5.8816], [15.347, 3.7104, 4.5135, 1.3822, 32.7767, 10.4977, 30.7198, 11.0662, 3.5199, 18.1712, 35.7145, 2.527, 37.9519, 43.221, 4.34, 0.541], [14.1577, 8.1184, 31.068, 17.3265, 0.1852, 9.6594, 3.6284, 31.1506, 54.6284, 16.9766, 13.8154, 3.1735, 23.7896, 17.5346, 1.4566, 9.3312]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.255346</td>\n",
       "      <td>0.400203</td>\n",
       "      <td>0.110489</td>\n",
       "      <td>0.861079</td>\n",
       "      <td>[10.8163, 8.159, 18.9494, 14.7458, 10.918, 15.6252, 19.1049, 15.2458, 15.2125, 6.2448, 14.5965, 22.3553, 14.2371, 18.3429, 32.6826, 18.7638]</td>\n",
       "      <td>[[0.007, 22.7081, 94.6559, 0.4965, 5.9408, 0.0, 4.3154, 0.5317, 12.7087, 0.0657, 0.2134, 0.0006, 11.8265, 14.3546, 0.0199, 88.1553], [0.0029, 1.3271, 0.061, 2.524, 4.1946, 0.0182, 33.5217, 34.9894, 13.1899, 0.5451, 3.4953, 48.2274, 0.0914, 17.5897, 96.2169, 0.0053], [32.4953, 0.0, 1.0627, 1.5117, 20.8277, 6.4678, 0.0006, 0.0, 17.381, 2.7116, 0.6096, 0.0475, 83.6366, 61.6313, 10.8312, 16.7855], [0.0264, 0.0106, 27.2333, 15.9385, 0.9402, 86.976, 9.1706, 30.109, 0.1653, 6.6049, 11.2175, 5.5914, 0.126, 35.8681, 11.9842, 14.0381], [1.9625, 3.2907, 15.0176, 70.6823, 0.1547, 45.6454, 1.1624, 4.4326, 1.3816, 2.932, 0.0979, 16.9121, 0.0006, 2.6125, 11.9256, 77.7896], [6.6465, 25.4103, 0.6225, 26.7438, 1.8312, 1.1249, 30.4988, 0.0, 8.2556, 13.6577, 0.9004, 85.0381, 0.0287, 3.2837, 48.5744, 3.3834], [3.6764, 0.0, 47.881, 4.3634, 15.6958, 1.6032, 36.2808, 4.027, 2.9138, 0.8195, 70.9719, 0.1735, 1.6729, 13.6758, 51.6923, 0.5528], [3.7814, 25.6659, 19.8775, 2.0451, 20.7644, 14.8376, 9.9619, 10.7796, 0.0996, 14.5662, 33.6671, 78.8798, 2.796, 5.9549, 0.524, 11.7989], [44.7562, 7.0047, 0.2128, 20.7614, 30.6301, 0.2948, 14.8646, 42.0064, 6.1401, 4.9355, 1.714, 0.9091, 17.7802, 0.4572, 59.5082, 4.0246], [5.8341, 7.0991, 4.5662, 5.1325, 3.49, 5.8242, 42.5082, 3.8933, 21.898, 6.1196, 23.7087, 19.8072, 17.0457, 5.8933, 81.3535, 1.8265], [8.3277, 1.068, 10.7919, 0.1038, 14.9449, 20.5574, 39.0328, 13.9004, 40.078, 7.3687, 8.5416, 11.061, 26.0457, 35.0217, 15.5932, 3.5633], [22.279, 4.3242, 5.4103, 26.6471, 11.6014, 4.153, 7.9408, 38.2802, 58.3382, 14.6114, 20.0211, 1.6161, 9.7948, 23.7714, 3.9683, 3.2427]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.193753</td>\n",
       "      <td>0.275860</td>\n",
       "      <td>0.111646</td>\n",
       "      <td>0.891559</td>\n",
       "      <td>[12.6164, 6.467, 17.5795, 16.1856, 9.9367, 16.2198, 29.0226, 14.2903, 14.4934, 8.2112, 23.3778, 11.8036, 15.6054, 22.0126, 28.1186, 10.0596]</td>\n",
       "      <td>[[0.0914, 25.6137, 85.7773, 0.0, 6.7415, 0.7685, 82.2761, 0.0018, 11.4326, 0.9637, 0.337, 2.2362, 10.2081, 0.4672, 0.0135, 29.0715], [0.7702, 0.2128, 6.0651, 18.6114, 12.8476, 0.0205, 22.466, 27.6565, 34.7749, 0.1448, 2.6653, 1.4771, 1.5117, 95.2374, 28.0387, 3.5], [38.8441, 0.7691, 0.5651, 6.8054, 1.0774, 1.1196, 0.3581, 0.0, 15.0826, 0.041, 87.5574, 1.2825, 69.4273, 11.6928, 7.8945, 13.483], [0.0147, 0.0176, 3.7087, 1.9789, 0.99, 58.2591, 3.6184, 15.4068, 0.0, 7.1237, 58.041, 11.6811, 0.0, 39.1061, 55.2778, 0.7761], [0.051, 0.0363, 22.0387, 90.6782, 1.8658, 34.1706, 1.0041, 37.7304, 2.3113, 1.0973, 0.0229, 22.255, 3.289, 6.3189, 2.3652, 30.7655], [9.561, 22.5311, 0.0, 29.721, 14.0481, 0.7491, 54.5774, 0.0, 3.4144, 14.415, 0.0492, 48.8875, 0.3558, 3.1823, 52.0996, 2.4086], [13.466, 0.0522, 54.0258, 11.9455, 4.1383, 3.5991, 2.2251, 0.5264, 3.2649, 3.3189, 60.3781, 0.1665, 3.68, 10.2896, 84.864, 0.0598], [0.0041, 9.2579, 26.9308, 0.8206, 32.2603, 43.1032, 40.4103, 0.0, 0.0, 44.6852, 42.9461, 4.905, 0.0, 8.7351, 0.3247, 1.6166], [32.8699, 14.7591, 0.2239, 4.4385, 14.0815, 0.1741, 0.0006, 41.5909, 5.1055, 8.4367, 0.0, 0.823, 36.8763, 0.8206, 60.9414, 34.8581], [41.915, 2.9566, 0.0, 8.4443, 6.3488, 7.6184, 72.7433, 3.3189, 1.4285, 15.303, 2.3171, 5.7567, 47.592, 0.0, 40.0856, 0.1717], [1.9766, 0.803, 11.0006, 0.0, 2.6389, 39.068, 45.4478, 4.0264, 34.1114, 0.0692, 10.7755, 39.1835, 12.2046, 48.058, 3.8546, 2.7819], [11.8329, 0.5944, 0.6184, 20.7837, 22.2022, 5.9871, 23.1442, 41.2251, 62.9947, 2.9355, 15.4443, 2.9894, 2.1196, 40.2427, 1.663, 1.2227]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.228585</td>\n",
       "      <td>0.347533</td>\n",
       "      <td>0.109636</td>\n",
       "      <td>0.882181</td>\n",
       "      <td>[7.9412, 10.6282, 14.1116, 12.3515, 19.1297, 16.9568, 22.641, 18.7249, 11.7642, 5.9914, 13.8163, 19.8625, 13.4665, 23.5871, 29.7349, 15.2923]</td>\n",
       "      <td>[[0.129, 26.041, 41.4308, 0.0, 6.6067, 1.6981, 59.3165, 0.0059, 7.8429, 0.0006, 0.6454, 0.296, 13.5633, 0.3212, 0.0258, 98.0768], [1.1389, 0.092, 1.1946, 0.4502, 0.4971, 0.425, 18.7866, 38.5, 4.9144, 0.4191, 1.0141, 52.2327, 1.9947, 61.8921, 71.9349, 0.5135], [21.126, 1.2567, 0.3822, 29.2737, 12.8652, 8.5293, 1.4924, 0.1208, 10.1981, 2.2438, 7.4601, 0.2597, 54.959, 86.2784, 12.4654, 7.0891], [0.5305, 22.2743, 1.0639, 3.6981, 0.5328, 51.5498, 1.0281, 26.0604, 0.0, 7.8769, 37.6577, 6.1512, 0.0, 7.7773, 64.7796, 25.0193], [0.0, 0.0363, 18.5914, 60.0879, 2.4062, 46.3664, 1.007, 56.204, 1.2327, 0.4596, 0.0006, 47.3816, 1.6161, 0.7233, 4.2233, 15.6635], [3.2562, 9.8124, 8.578, 2.8523, 56.1389, 0.1659, 51.4232, 0.0, 2.5117, 11.5358, 0.0, 54.5393, 0.1184, 4.8974, 48.459, 1.7116], [5.2456, 0.0047, 61.5023, 2.9596, 34.5586, 9.3804, 13.8394, 0.8447, 9.8681, 0.9543, 47.2591, 0.279, 12.4666, 16.7186, 40.1125, 0.0064], [0.2198, 25.197, 28.6934, 1.7784, 20.5492, 37.1483, 34.7884, 0.0, 0.0, 8.3335, 44.4596, 37.857, 0.0, 16.3792, 0.2825, 0.3136], [33.7433, 0.0, 2.2192, 2.7521, 45.1659, 0.0, 0.0, 43.2081, 9.7978, 4.2245, 0.0, 1.5662, 6.5885, 5.2433, 71.6055, 29.8857], [0.9818, 36.1401, 1.0322, 12.2638, 14.5592, 0.7251, 72.8552, 6.575, 0.1981, 29.3447, 0.1284, 5.9959, 39.8183, 2.4132, 31.0053, 1.9637], [2.7761, 5.4045, 4.5961, 1.7479, 0.5674, 44.2919, 12.2169, 3.5193, 38.4525, 0.0023, 24.7497, 29.8998, 26.1477, 47.8945, 11.8728, 1.8605], [26.1471, 1.279, 0.0545, 30.354, 35.109, 3.2016, 4.9379, 49.6612, 56.1536, 6.5012, 2.4215, 1.8921, 4.3253, 32.507, 0.0516, 1.4033]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.207781</td>\n",
       "      <td>0.305514</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>0.886284</td>\n",
       "      <td>[9.7218, 12.8265, 10.3774, 11.7386, 22.1044, 19.7308, 27.6791, 11.9632, 10.2061, 7.3607, 17.8356, 11.2287, 11.0628, 27.2823, 28.2512, 16.6309]</td>\n",
       "      <td>[[0.5674, 25.2122, 24.6987, 0.1864, 6.0973, 0.0, 92.0698, 0.0029, 10.1295, 0.0023, 0.7309, 0.5012, 14.6149, 0.6964, 0.0158, 80.4742], [0.0, 0.2907, 1.3077, 6.7778, 0.374, 0.0023, 32.762, 22.1758, 5.2403, 0.1594, 0.0955, 46.2995, 2.0463, 115.7333, 22.6032, 0.1319], [50.1794, 0.0, 0.1272, 24.4918, 9.1641, 0.2403, 0.357, 0.1301, 11.1647, 0.0094, 0.7562, 0.3259, 37.0914, 105.5932, 9.612, 6.7573], [0.0164, 19.9308, 0.6524, 0.4771, 0.2978, 61.9209, 4.6788, 5.8576, 0.0, 5.8118, 74.2884, 5.483, 0.0, 12.9103, 62.4449, 1.2298], [0.0, 0.0621, 9.7157, 46.1858, 0.3447, 68.6852, 1.0475, 16.9297, 14.541, 0.1155, 0.0018, 15.6184, 0.0, 2.1882, 9.0938, 71.4707], [0.7022, 19.095, 3.2386, 6.1712, 53.5504, 0.2263, 60.6758, 0.0, 1.1225, 18.5457, 0.2128, 31.6893, 0.0633, 12.3118, 48.3939, 0.0012], [1.9127, 0.0193, 49.0346, 5.7075, 70.7134, 3.3394, 5.7216, 2.8171, 5.527, 1.3277, 51.8324, 0.2667, 1.405, 16.9531, 39.4156, 0.007], [1.3646, 9.8669, 29.9115, 1.5739, 33.6589, 44.3288, 39.3312, 0.551, 1.5897, 8.7579, 52.6729, 1.0868, 0.0, 8.6231, 16.609, 6.0739], [36.1805, 3.7655, 0.0, 4.7128, 54.6354, 0.5727, 0.0533, 44.527, 2.2691, 3.956, 0.0, 0.0023, 5.2403, 14.9513, 57.1876, 27.9461], [2.5182, 46.7784, 0.0363, 18.6061, 12.6178, 0.2491, 62.6073, 1.5258, 1.0709, 22.9601, 0.0, 6.075, 47.7257, 2.2145, 30.1067, 0.908], [2.9924, 15.8816, 5.6032, 0.7814, 1.6073, 53.6231, 29.6401, 5.6225, 20.4543, 0.0012, 20.075, 8.0041, 20.1747, 26.4977, 43.1114, 1.9302], [20.228, 13.0152, 0.2028, 25.1911, 22.1923, 3.5815, 3.2046, 43.4191, 49.364, 26.6811, 13.3611, 19.3921, 4.3916, 8.7145, 0.4203, 2.6407]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.225586</td>\n",
       "      <td>0.346491</td>\n",
       "      <td>0.104681</td>\n",
       "      <td>0.888042</td>\n",
       "      <td>[12.5136, 8.5907, 10.9527, 12.2519, 17.0731, 18.1158, 27.8012, 14.4875, 11.7695, 11.3764, 18.7212, 13.5471, 13.8542, 25.6154, 21.9054, 17.4243]</td>\n",
       "      <td>[[0.6301, 25.1665, 55.9713, 0.8453, 3.119, 0.1213, 84.6958, 0.0023, 13.0516, 0.1823, 0.2614, 0.4607, 15.8458, 0.4871, 0.0047, 55.1547], [0.0, 1.796, 1.1788, 25.0768, 0.6336, 0.1811, 34.442, 29.2374, 13.0404, 0.4455, 0.2327, 44.3177, 0.6208, 73.6987, 30.4185, 0.68], [52.7743, 0.1213, 0.2005, 8.493, 19.7134, 0.1096, 0.3529, 0.1589, 10.7098, 0.0047, 4.1395, 0.1653, 62.6723, 91.7972, 0.2538, 4.3335], [0.0727, 20.6243, 1.8341, 4.6682, 0.7497, 62.7603, 0.0006, 8.34, 0.0, 12.9549, 55.7573, 7.8822, 0.0, 26.9379, 45.3957, 8.0223], [0.0, 0.0, 8.7732, 40.8599, 2.7556, 55.5252, 1.6272, 23.2526, 20.0363, 3.2485, 0.0023, 24.3406, 0.1794, 0.7732, 12.5047, 62.1213], [5.7421, 9.0358, 0.0, 1.9009, 77.6835, 0.5885, 50.4566, 0.0, 2.8687, 43.9601, 2.0569, 31.0574, 1.0399, 15.9683, 13.5199, 0.1213], [0.3951, 0.0135, 40.0293, 5.7192, 30.6653, 5.8453, 25.9496, 2.7978, 13.0504, 2.228, 55.6571, 0.3781, 0.0, 11.5703, 61.6899, 0.0111], [2.5574, 7.0076, 9.4678, 4.0487, 28.2198, 51.772, 47.6958, 0.0551, 0.0012, 12.303, 34.4642, 0.4912, 0.0, 39.2515, 10.9385, 7.7263], [29.4361, 21.9725, 0.3617, 14.456, 15.1465, 0.5979, 1.0903, 21.8839, 5.5158, 12.5891, 31.1659, 0.0006, 20.7802, 4.0498, 44.6014, 32.3523], [30.3007, 7.7222, 0.0557, 8.0838, 6.2485, 9.7433, 43.8798, 18.0275, 0.7585, 19.0481, 4.0487, 16.2397, 34.129, 4.061, 31.1981, 22.4555], [4.4443, 1.303, 9.4385, 0.1923, 7.0586, 28.4666, 27.2591, 38.4543, 21.1032, 0.0006, 30.7304, 13.3025, 21.5586, 28.18, 12.1987, 12.3095], [23.8107, 8.3253, 4.1213, 32.6782, 12.8834, 1.6782, 16.1647, 31.6407, 41.0979, 29.5522, 6.1383, 23.9297, 9.4244, 10.6096, 0.1413, 3.8042]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.225402</td>\n",
       "      <td>0.345454</td>\n",
       "      <td>0.105350</td>\n",
       "      <td>0.885111</td>\n",
       "      <td>[14.1208, 9.5362, 9.2585, 14.0748, 16.0394, 17.8002, 24.7877, 14.109, 12.0541, 12.1739, 20.6892, 13.5276, 15.8872, 26.3602, 18.5508, 17.0301]</td>\n",
       "      <td>[[0.6635, 25.3447, 37.7339, 0.5487, 2.8916, 0.1213, 88.3892, 0.0094, 12.4349, 1.0223, 0.9484, 0.3511, 14.8558, 0.8036, 0.0047, 69.8769], [0.1213, 2.9478, 0.9191, 30.68, 0.4437, 0.4009, 19.9543, 23.6905, 5.3288, 0.3769, 0.9801, 48.8576, 2.507, 105.8441, 12.3921, 0.5557], [66.5557, 0.1729, 0.1026, 22.3535, 14.5147, 0.0574, 0.3998, 0.1788, 13.5715, 0.0041, 2.1829, 0.0399, 57.2022, 72.7544, 0.3998, 5.51], [0.0141, 23.4824, 1.7995, 0.4818, 0.4625, 52.1178, 0.0, 11.7526, 0.0, 7.3335, 80.5692, 5.8189, 0.0, 27.1717, 42.1706, 2.8253], [0.0, 0.0, 8.8734, 43.0023, 1.1336, 53.8664, 1.4484, 21.9578, 11.4174, 0.7978, 0.0018, 19.041, 0.1794, 1.17, 9.6219, 83.4889], [5.4121, 29.6917, 6.2098, 1.3687, 77.8277, 0.2433, 46.7702, 0.0, 4.0141, 37.0293, 1.0129, 22.7591, 1.1313, 13.6243, 8.6987, 0.2069], [0.6125, 0.0275, 38.8962, 17.534, 14.8921, 6.9965, 16.4121, 6.1893, 26.3834, 1.9519, 51.1518, 0.0064, 0.0, 7.9683, 66.9162, 0.0615], [25.0604, 4.9631, 6.9396, 2.0868, 28.8142, 47.1278, 53.1764, 0.0551, 0.0, 9.5686, 32.9496, 0.2872, 0.0, 31.9812, 4.2972, 8.6928], [21.5375, 7.2814, 0.0, 11.007, 13.4918, 0.2831, 0.1671, 29.7819, 13.973, 26.6583, 45.0223, 0.0035, 35.4127, 2.7421, 41.3775, 7.2608], [22.6131, 5.8886, 0.0533, 9.5739, 8.0041, 8.5604, 39.1026, 12.1424, 0.922, 37.1489, 2.3189, 17.1841, 46.5123, 3.956, 22.7667, 19.2526], [0.9654, 3.9132, 6.0123, 0.1319, 5.3036, 43.1823, 19.3693, 34.7374, 19.8916, 0.0006, 23.4701, 26.5873, 25.078, 27.3605, 13.9642, 6.0322], [25.8933, 10.7216, 3.5627, 30.1295, 24.6934, 0.6454, 12.2632, 28.8124, 36.7128, 24.1952, 7.663, 21.3957, 7.7679, 20.9467, 0.0, 0.5973]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[23.9297, 13.7951, 9.2984, 11.6721, 11.4894, 19.1574, 20.97, 17.5376, 16.7266, 15.8474, 16.8836, 21.9895, 18.5515, 13.2515, 17.7794, 7.121]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.6934, 25.8136, 4.939, 83.8986, 13.4988, 0.0, 0.7456, 0.2808, 3.7948, 89.7397, 5.1161, 2.4719, 5.9461, 19.0188, 0.027, 0.0158], [90.3083, 93.9789, 0.3224, 0.8992, 0.3476, 0.1032, 0.0035, 28.9027, 0.5651, 1.1606, 12.7673, 14.5563, 2.3775, 0.1483, 1.6172, 7.942], [102.2755, 0.0311, 1.2732, 2.4601, 0.0064, 0.7169, 0.0, 0.0346, 10.4256, 21.0516, 81.7544, 0.0545, 10.1243, 10.337, 0.7339, 14.721], [0.0, 0.0018, 0.0064, 7.4631, 0.0, 65.6184, 6.6172, 10.2655, 8.0703, 12.5258, 0.0903, 11.4742, 1.1917, 60.1577, 72.4332, 0.0844], [0.1325, 9.9725, 3.5762, 2.2884, 24.49, 18.871, 1.4402, 11.9097, 0.1712, 5.6565, 10.7649, 112.8593, 7.0803, 3.4543, 34.9959, 8.337], [7.221, 24.5012, 18.4994, 3.558, 0.1002, 77.272, 26.9074, 0.7052, 0.0006, 5.8054, 0.9678, 8.4285, 55.4056, 7.2198, 7.0035, 12.4045], [5.0457, 0.0, 53.6753, 9.9988, 0.32, 9.422, 13.2696, 63.9596, 45.3066, 0.0305, 4.8916, 13.4637, 0.0012, 5.2016, 31.4138, 0.0], [0.0569, 8.3001, 0.8347, 0.3599, 29.7526, 16.374, 20.6213, 50.745, 0.1366, 0.0, 26.1383, 40.541, 42.3986, 2.9426, 0.136, 16.6624], [20.3828, 0.0006, 0.2075, 11.5563, 42.2937, 15.6102, 1.3271, 1.6166, 53.7098, 3.9965, 42.8283, 0.0604, 12.313, 0.1395, 46.2444, 3.7134], [0.0, 1.8681, 4.7831, 5.1413, 7.9232, 23.6125, 72.6483, 1.9555, 0.0182, 48.4625, 17.2585, 35.5809, 21.1841, 0.0, 0.0041, 15.5598], [16.0229, 1.0733, 23.4631, 12.4414, 18.4912, 2.289, 46.296, 39.9004, 3.6084, 0.0, 0.0217, 24.3834, 5.6231, 39.3535, 18.7433, 4.2896], [45.017, 0.0, 0.0, 0.0, 0.6483, 0.0, 61.7632, 0.1758, 74.9115, 1.7397, 0.0047, 0.0, 58.973, 11.0445, 0.0, 1.7222]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-214\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-214/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-214/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-214/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-214/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-214/special_tokens_map.json\n",
      "Trainer is attempting to log a value of \"[8.4195, 9.5398, 18.2963, 15.9447, 16.9168, 16.2247, 20.1295, 16.7506, 19.6154, 11.5378, 26.8729, 8.6438, 14.7053, 24.5061, 12.8042, 15.0927]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.0147, 24.2691, 82.0076, 3.908, 8.8224, 0.0, 40.8453, 33.0574, 18.51, 0.0, 0.0123, 0.323, 4.0358, 0.4004, 5.7052, 34.0891], [0.0604, 2.0199, 0.1841, 17.4543, 11.1483, 1.6442, 0.7005, 25.7327, 67.371, 17.5293, 61.0281, 0.5012, 0.5574, 44.0264, 3.0059, 3.0363], [8.0557, 1.0076, 0.2122, 5.1372, 42.5516, 3.517, 12.5721, 10.1348, 28.2767, 0.0123, 8.0, 1.078, 87.1817, 9.2743, 28.3277, 10.6612], [0.0012, 0.0938, 0.6213, 0.0, 0.0, 30.9496, 1.5698, 7.8341, 15.8013, 12.3353, 73.6489, 2.3177, 5.6571, 104.3927, 0.1342, 0.643], [0.4607, 16.0727, 1.6284, 56.7034, 24.296, 66.9777, 10.9338, 4.9513, 0.3828, 10.17, 1.8118, 3.8136, 0.6049, 16.6254, 26.0615, 14.5059], [6.1934, 25.7597, 23.4226, 23.2057, 2.7737, 15.1964, 33.4132, 0.0, 0.0, 8.5797, 3.9871, 41.561, 0.1366, 15.3236, 38.1676, 18.2796], [3.0826, 0.9273, 39.7579, 8.1043, 8.0305, 29.7198, 32.5334, 15.2362, 32.1055, 2.364, 63.3687, 1.4179, 3.711, 9.8066, 4.2028, 1.6313], [12.9941, 3.5029, 4.5305, 2.2409, 19.7421, 22.1952, 19.8705, 9.4543, 3.4068, 34.1032, 38.9209, 11.2907, 2.0657, 11.262, 0.1096, 60.3107], [10.7773, 10.4179, 2.6225, 36.1659, 14.0674, 0.786, 15.2315, 50.7884, 0.9683, 17.857, 3.4607, 8.4478, 4.493, 19.7474, 37.9678, 22.2011], [29.8892, 18.578, 28.9865, 19.7081, 38.6079, 3.5533, 39.5358, 1.6008, 10.4144, 0.3546, 18.7069, 27.2737, 6.2784, 2.459, 4.1717, 5.8816], [15.347, 3.7104, 4.5135, 1.3822, 32.7767, 10.4977, 30.7198, 11.0662, 3.5199, 18.1712, 35.7145, 2.527, 37.9519, 43.221, 4.34, 0.541], [14.1577, 8.1184, 31.068, 17.3265, 0.1852, 9.6594, 3.6284, 31.1506, 54.6284, 16.9766, 13.8154, 3.1735, 23.7896, 17.5346, 1.4566, 9.3312]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-428\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-428/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-428/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-428/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-428/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-428/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-214] due to args.save_total_limit\n",
      "Trainer is attempting to log a value of \"[10.8163, 8.159, 18.9494, 14.7458, 10.918, 15.6252, 19.1049, 15.2458, 15.2125, 6.2448, 14.5965, 22.3553, 14.2371, 18.3429, 32.6826, 18.7638]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.007, 22.7081, 94.6559, 0.4965, 5.9408, 0.0, 4.3154, 0.5317, 12.7087, 0.0657, 0.2134, 0.0006, 11.8265, 14.3546, 0.0199, 88.1553], [0.0029, 1.3271, 0.061, 2.524, 4.1946, 0.0182, 33.5217, 34.9894, 13.1899, 0.5451, 3.4953, 48.2274, 0.0914, 17.5897, 96.2169, 0.0053], [32.4953, 0.0, 1.0627, 1.5117, 20.8277, 6.4678, 0.0006, 0.0, 17.381, 2.7116, 0.6096, 0.0475, 83.6366, 61.6313, 10.8312, 16.7855], [0.0264, 0.0106, 27.2333, 15.9385, 0.9402, 86.976, 9.1706, 30.109, 0.1653, 6.6049, 11.2175, 5.5914, 0.126, 35.8681, 11.9842, 14.0381], [1.9625, 3.2907, 15.0176, 70.6823, 0.1547, 45.6454, 1.1624, 4.4326, 1.3816, 2.932, 0.0979, 16.9121, 0.0006, 2.6125, 11.9256, 77.7896], [6.6465, 25.4103, 0.6225, 26.7438, 1.8312, 1.1249, 30.4988, 0.0, 8.2556, 13.6577, 0.9004, 85.0381, 0.0287, 3.2837, 48.5744, 3.3834], [3.6764, 0.0, 47.881, 4.3634, 15.6958, 1.6032, 36.2808, 4.027, 2.9138, 0.8195, 70.9719, 0.1735, 1.6729, 13.6758, 51.6923, 0.5528], [3.7814, 25.6659, 19.8775, 2.0451, 20.7644, 14.8376, 9.9619, 10.7796, 0.0996, 14.5662, 33.6671, 78.8798, 2.796, 5.9549, 0.524, 11.7989], [44.7562, 7.0047, 0.2128, 20.7614, 30.6301, 0.2948, 14.8646, 42.0064, 6.1401, 4.9355, 1.714, 0.9091, 17.7802, 0.4572, 59.5082, 4.0246], [5.8341, 7.0991, 4.5662, 5.1325, 3.49, 5.8242, 42.5082, 3.8933, 21.898, 6.1196, 23.7087, 19.8072, 17.0457, 5.8933, 81.3535, 1.8265], [8.3277, 1.068, 10.7919, 0.1038, 14.9449, 20.5574, 39.0328, 13.9004, 40.078, 7.3687, 8.5416, 11.061, 26.0457, 35.0217, 15.5932, 3.5633], [22.279, 4.3242, 5.4103, 26.6471, 11.6014, 4.153, 7.9408, 38.2802, 58.3382, 14.6114, 20.0211, 1.6161, 9.7948, 23.7714, 3.9683, 3.2427]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-642\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-642/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-642/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-642/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-642/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-642/special_tokens_map.json\n",
      "Trainer is attempting to log a value of \"[12.6164, 6.467, 17.5795, 16.1856, 9.9367, 16.2198, 29.0226, 14.2903, 14.4934, 8.2112, 23.3778, 11.8036, 15.6054, 22.0126, 28.1186, 10.0596]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.0914, 25.6137, 85.7773, 0.0, 6.7415, 0.7685, 82.2761, 0.0018, 11.4326, 0.9637, 0.337, 2.2362, 10.2081, 0.4672, 0.0135, 29.0715], [0.7702, 0.2128, 6.0651, 18.6114, 12.8476, 0.0205, 22.466, 27.6565, 34.7749, 0.1448, 2.6653, 1.4771, 1.5117, 95.2374, 28.0387, 3.5], [38.8441, 0.7691, 0.5651, 6.8054, 1.0774, 1.1196, 0.3581, 0.0, 15.0826, 0.041, 87.5574, 1.2825, 69.4273, 11.6928, 7.8945, 13.483], [0.0147, 0.0176, 3.7087, 1.9789, 0.99, 58.2591, 3.6184, 15.4068, 0.0, 7.1237, 58.041, 11.6811, 0.0, 39.1061, 55.2778, 0.7761], [0.051, 0.0363, 22.0387, 90.6782, 1.8658, 34.1706, 1.0041, 37.7304, 2.3113, 1.0973, 0.0229, 22.255, 3.289, 6.3189, 2.3652, 30.7655], [9.561, 22.5311, 0.0, 29.721, 14.0481, 0.7491, 54.5774, 0.0, 3.4144, 14.415, 0.0492, 48.8875, 0.3558, 3.1823, 52.0996, 2.4086], [13.466, 0.0522, 54.0258, 11.9455, 4.1383, 3.5991, 2.2251, 0.5264, 3.2649, 3.3189, 60.3781, 0.1665, 3.68, 10.2896, 84.864, 0.0598], [0.0041, 9.2579, 26.9308, 0.8206, 32.2603, 43.1032, 40.4103, 0.0, 0.0, 44.6852, 42.9461, 4.905, 0.0, 8.7351, 0.3247, 1.6166], [32.8699, 14.7591, 0.2239, 4.4385, 14.0815, 0.1741, 0.0006, 41.5909, 5.1055, 8.4367, 0.0, 0.823, 36.8763, 0.8206, 60.9414, 34.8581], [41.915, 2.9566, 0.0, 8.4443, 6.3488, 7.6184, 72.7433, 3.3189, 1.4285, 15.303, 2.3171, 5.7567, 47.592, 0.0, 40.0856, 0.1717], [1.9766, 0.803, 11.0006, 0.0, 2.6389, 39.068, 45.4478, 4.0264, 34.1114, 0.0692, 10.7755, 39.1835, 12.2046, 48.058, 3.8546, 2.7819], [11.8329, 0.5944, 0.6184, 20.7837, 22.2022, 5.9871, 23.1442, 41.2251, 62.9947, 2.9355, 15.4443, 2.9894, 2.1196, 40.2427, 1.663, 1.2227]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-856\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-856/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-856/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-856/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-856/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-856/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-428] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-642] due to args.save_total_limit\n",
      "Trainer is attempting to log a value of \"[7.9412, 10.6282, 14.1116, 12.3515, 19.1297, 16.9568, 22.641, 18.7249, 11.7642, 5.9914, 13.8163, 19.8625, 13.4665, 23.5871, 29.7349, 15.2923]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.129, 26.041, 41.4308, 0.0, 6.6067, 1.6981, 59.3165, 0.0059, 7.8429, 0.0006, 0.6454, 0.296, 13.5633, 0.3212, 0.0258, 98.0768], [1.1389, 0.092, 1.1946, 0.4502, 0.4971, 0.425, 18.7866, 38.5, 4.9144, 0.4191, 1.0141, 52.2327, 1.9947, 61.8921, 71.9349, 0.5135], [21.126, 1.2567, 0.3822, 29.2737, 12.8652, 8.5293, 1.4924, 0.1208, 10.1981, 2.2438, 7.4601, 0.2597, 54.959, 86.2784, 12.4654, 7.0891], [0.5305, 22.2743, 1.0639, 3.6981, 0.5328, 51.5498, 1.0281, 26.0604, 0.0, 7.8769, 37.6577, 6.1512, 0.0, 7.7773, 64.7796, 25.0193], [0.0, 0.0363, 18.5914, 60.0879, 2.4062, 46.3664, 1.007, 56.204, 1.2327, 0.4596, 0.0006, 47.3816, 1.6161, 0.7233, 4.2233, 15.6635], [3.2562, 9.8124, 8.578, 2.8523, 56.1389, 0.1659, 51.4232, 0.0, 2.5117, 11.5358, 0.0, 54.5393, 0.1184, 4.8974, 48.459, 1.7116], [5.2456, 0.0047, 61.5023, 2.9596, 34.5586, 9.3804, 13.8394, 0.8447, 9.8681, 0.9543, 47.2591, 0.279, 12.4666, 16.7186, 40.1125, 0.0064], [0.2198, 25.197, 28.6934, 1.7784, 20.5492, 37.1483, 34.7884, 0.0, 0.0, 8.3335, 44.4596, 37.857, 0.0, 16.3792, 0.2825, 0.3136], [33.7433, 0.0, 2.2192, 2.7521, 45.1659, 0.0, 0.0, 43.2081, 9.7978, 4.2245, 0.0, 1.5662, 6.5885, 5.2433, 71.6055, 29.8857], [0.9818, 36.1401, 1.0322, 12.2638, 14.5592, 0.7251, 72.8552, 6.575, 0.1981, 29.3447, 0.1284, 5.9959, 39.8183, 2.4132, 31.0053, 1.9637], [2.7761, 5.4045, 4.5961, 1.7479, 0.5674, 44.2919, 12.2169, 3.5193, 38.4525, 0.0023, 24.7497, 29.8998, 26.1477, 47.8945, 11.8728, 1.8605], [26.1471, 1.279, 0.0545, 30.354, 35.109, 3.2016, 4.9379, 49.6612, 56.1536, 6.5012, 2.4215, 1.8921, 4.3253, 32.507, 0.0516, 1.4033]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1070\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1070/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1070/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1070/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1070/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1070/special_tokens_map.json\n",
      "Trainer is attempting to log a value of \"[9.7218, 12.8265, 10.3774, 11.7386, 22.1044, 19.7308, 27.6791, 11.9632, 10.2061, 7.3607, 17.8356, 11.2287, 11.0628, 27.2823, 28.2512, 16.6309]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.5674, 25.2122, 24.6987, 0.1864, 6.0973, 0.0, 92.0698, 0.0029, 10.1295, 0.0023, 0.7309, 0.5012, 14.6149, 0.6964, 0.0158, 80.4742], [0.0, 0.2907, 1.3077, 6.7778, 0.374, 0.0023, 32.762, 22.1758, 5.2403, 0.1594, 0.0955, 46.2995, 2.0463, 115.7333, 22.6032, 0.1319], [50.1794, 0.0, 0.1272, 24.4918, 9.1641, 0.2403, 0.357, 0.1301, 11.1647, 0.0094, 0.7562, 0.3259, 37.0914, 105.5932, 9.612, 6.7573], [0.0164, 19.9308, 0.6524, 0.4771, 0.2978, 61.9209, 4.6788, 5.8576, 0.0, 5.8118, 74.2884, 5.483, 0.0, 12.9103, 62.4449, 1.2298], [0.0, 0.0621, 9.7157, 46.1858, 0.3447, 68.6852, 1.0475, 16.9297, 14.541, 0.1155, 0.0018, 15.6184, 0.0, 2.1882, 9.0938, 71.4707], [0.7022, 19.095, 3.2386, 6.1712, 53.5504, 0.2263, 60.6758, 0.0, 1.1225, 18.5457, 0.2128, 31.6893, 0.0633, 12.3118, 48.3939, 0.0012], [1.9127, 0.0193, 49.0346, 5.7075, 70.7134, 3.3394, 5.7216, 2.8171, 5.527, 1.3277, 51.8324, 0.2667, 1.405, 16.9531, 39.4156, 0.007], [1.3646, 9.8669, 29.9115, 1.5739, 33.6589, 44.3288, 39.3312, 0.551, 1.5897, 8.7579, 52.6729, 1.0868, 0.0, 8.6231, 16.609, 6.0739], [36.1805, 3.7655, 0.0, 4.7128, 54.6354, 0.5727, 0.0533, 44.527, 2.2691, 3.956, 0.0, 0.0023, 5.2403, 14.9513, 57.1876, 27.9461], [2.5182, 46.7784, 0.0363, 18.6061, 12.6178, 0.2491, 62.6073, 1.5258, 1.0709, 22.9601, 0.0, 6.075, 47.7257, 2.2145, 30.1067, 0.908], [2.9924, 15.8816, 5.6032, 0.7814, 1.6073, 53.6231, 29.6401, 5.6225, 20.4543, 0.0012, 20.075, 8.0041, 20.1747, 26.4977, 43.1114, 1.9302], [20.228, 13.0152, 0.2028, 25.1911, 22.1923, 3.5815, 3.2046, 43.4191, 49.364, 26.6811, 13.3611, 19.3921, 4.3916, 8.7145, 0.4203, 2.6407]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1284\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1284/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1284/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1284/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1284/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1284/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1070] due to args.save_total_limit\n",
      "Trainer is attempting to log a value of \"[12.5136, 8.5907, 10.9527, 12.2519, 17.0731, 18.1158, 27.8012, 14.4875, 11.7695, 11.3764, 18.7212, 13.5471, 13.8542, 25.6154, 21.9054, 17.4243]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.6301, 25.1665, 55.9713, 0.8453, 3.119, 0.1213, 84.6958, 0.0023, 13.0516, 0.1823, 0.2614, 0.4607, 15.8458, 0.4871, 0.0047, 55.1547], [0.0, 1.796, 1.1788, 25.0768, 0.6336, 0.1811, 34.442, 29.2374, 13.0404, 0.4455, 0.2327, 44.3177, 0.6208, 73.6987, 30.4185, 0.68], [52.7743, 0.1213, 0.2005, 8.493, 19.7134, 0.1096, 0.3529, 0.1589, 10.7098, 0.0047, 4.1395, 0.1653, 62.6723, 91.7972, 0.2538, 4.3335], [0.0727, 20.6243, 1.8341, 4.6682, 0.7497, 62.7603, 0.0006, 8.34, 0.0, 12.9549, 55.7573, 7.8822, 0.0, 26.9379, 45.3957, 8.0223], [0.0, 0.0, 8.7732, 40.8599, 2.7556, 55.5252, 1.6272, 23.2526, 20.0363, 3.2485, 0.0023, 24.3406, 0.1794, 0.7732, 12.5047, 62.1213], [5.7421, 9.0358, 0.0, 1.9009, 77.6835, 0.5885, 50.4566, 0.0, 2.8687, 43.9601, 2.0569, 31.0574, 1.0399, 15.9683, 13.5199, 0.1213], [0.3951, 0.0135, 40.0293, 5.7192, 30.6653, 5.8453, 25.9496, 2.7978, 13.0504, 2.228, 55.6571, 0.3781, 0.0, 11.5703, 61.6899, 0.0111], [2.5574, 7.0076, 9.4678, 4.0487, 28.2198, 51.772, 47.6958, 0.0551, 0.0012, 12.303, 34.4642, 0.4912, 0.0, 39.2515, 10.9385, 7.7263], [29.4361, 21.9725, 0.3617, 14.456, 15.1465, 0.5979, 1.0903, 21.8839, 5.5158, 12.5891, 31.1659, 0.0006, 20.7802, 4.0498, 44.6014, 32.3523], [30.3007, 7.7222, 0.0557, 8.0838, 6.2485, 9.7433, 43.8798, 18.0275, 0.7585, 19.0481, 4.0487, 16.2397, 34.129, 4.061, 31.1981, 22.4555], [4.4443, 1.303, 9.4385, 0.1923, 7.0586, 28.4666, 27.2591, 38.4543, 21.1032, 0.0006, 30.7304, 13.3025, 21.5586, 28.18, 12.1987, 12.3095], [23.8107, 8.3253, 4.1213, 32.6782, 12.8834, 1.6782, 16.1647, 31.6407, 41.0979, 29.5522, 6.1383, 23.9297, 9.4244, 10.6096, 0.1413, 3.8042]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1498\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1498/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1498/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1498/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1498/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1498/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1284] due to args.save_total_limit\n",
      "Trainer is attempting to log a value of \"[14.1208, 9.5362, 9.2585, 14.0748, 16.0394, 17.8002, 24.7877, 14.109, 12.0541, 12.1739, 20.6892, 13.5276, 15.8872, 26.3602, 18.5508, 17.0301]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.6635, 25.3447, 37.7339, 0.5487, 2.8916, 0.1213, 88.3892, 0.0094, 12.4349, 1.0223, 0.9484, 0.3511, 14.8558, 0.8036, 0.0047, 69.8769], [0.1213, 2.9478, 0.9191, 30.68, 0.4437, 0.4009, 19.9543, 23.6905, 5.3288, 0.3769, 0.9801, 48.8576, 2.507, 105.8441, 12.3921, 0.5557], [66.5557, 0.1729, 0.1026, 22.3535, 14.5147, 0.0574, 0.3998, 0.1788, 13.5715, 0.0041, 2.1829, 0.0399, 57.2022, 72.7544, 0.3998, 5.51], [0.0141, 23.4824, 1.7995, 0.4818, 0.4625, 52.1178, 0.0, 11.7526, 0.0, 7.3335, 80.5692, 5.8189, 0.0, 27.1717, 42.1706, 2.8253], [0.0, 0.0, 8.8734, 43.0023, 1.1336, 53.8664, 1.4484, 21.9578, 11.4174, 0.7978, 0.0018, 19.041, 0.1794, 1.17, 9.6219, 83.4889], [5.4121, 29.6917, 6.2098, 1.3687, 77.8277, 0.2433, 46.7702, 0.0, 4.0141, 37.0293, 1.0129, 22.7591, 1.1313, 13.6243, 8.6987, 0.2069], [0.6125, 0.0275, 38.8962, 17.534, 14.8921, 6.9965, 16.4121, 6.1893, 26.3834, 1.9519, 51.1518, 0.0064, 0.0, 7.9683, 66.9162, 0.0615], [25.0604, 4.9631, 6.9396, 2.0868, 28.8142, 47.1278, 53.1764, 0.0551, 0.0, 9.5686, 32.9496, 0.2872, 0.0, 31.9812, 4.2972, 8.6928], [21.5375, 7.2814, 0.0, 11.007, 13.4918, 0.2831, 0.1671, 29.7819, 13.973, 26.6583, 45.0223, 0.0035, 35.4127, 2.7421, 41.3775, 7.2608], [22.6131, 5.8886, 0.0533, 9.5739, 8.0041, 8.5604, 39.1026, 12.1424, 0.922, 37.1489, 2.3189, 17.1841, 46.5123, 3.956, 22.7667, 19.2526], [0.9654, 3.9132, 6.0123, 0.1319, 5.3036, 43.1823, 19.3693, 34.7374, 19.8916, 0.0006, 23.4701, 26.5873, 25.078, 27.3605, 13.9642, 6.0322], [25.8933, 10.7216, 3.5627, 30.1295, 24.6934, 0.6454, 12.2632, 28.8124, 36.7128, 24.1952, 7.663, 21.3957, 7.7679, 20.9467, 0.0, 0.5973]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1712\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1712/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1712/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1712/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1712/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1712/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1498] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-856 (score: 0.19375303015112877).\n",
      "Deleting older checkpoint [/home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/checkpoint-1712] due to args.save_total_limit\n",
      "Saving model checkpoint to /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/config.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/generation_config.json\n",
      "Model weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/pytorch_model.bin\n",
      "tokenizer config file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/tokenizer_config.json\n",
      "Special tokens file saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/special_tokens_map.json\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_gating_network/moe_sentiment_rotten_tomatoes_16E/gating_network_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_gating_network/moe_sentiment_rotten_tomatoes_16E/pytorch_gating_network.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_0/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_0/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_1/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_1/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_2/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_2/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_3/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_3/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_4/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_4/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_5/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_5/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_6/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_6/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_7/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_7/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_8/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_8/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_9/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_9/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_10/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_10/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_11/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_11/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_12/adapter_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  total_flos               =  3937761GF\n",
      "  train_loss               =     0.1366\n",
      "  train_runtime            = 0:40:45.57\n",
      "  train_samples_per_second =     27.903\n",
      "  train_steps_per_second   =      0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_12/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_13/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_13/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_14/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_14/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_15/adapter_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_adapters/expert_15/pytorch_adapter.bin\n",
      "Configuration saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_head/moe_sentiment_rotten_tomatoes_16E/head_config.json\n",
      "Module weights saved in /home/jaehan/research/adapter/adapter-poisoning/data_ign/tmp_case3_moeBaseline/moe_sentiment_rotten_tomatoes_16E_20231218-161224/trained_head/moe_sentiment_rotten_tomatoes_16E/pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "\n",
    "loss_history = {'base_model': model_name_or_path,\n",
    "                'max_seq_length': max_seq_length,\n",
    "                'random_seed': random_seed,\n",
    "                'lr': learning_rate,\n",
    "                'warmup_ratio': warmup_ratio,\n",
    "                'early_stopping_patience': patience,\n",
    "                'total_batch_size': total_batch_size_train,\n",
    "                'num_train_epoch': num_train_epochs,\n",
    "                'adapter_count': adapter_count,\n",
    "                'adapter_k': adapter_k,\n",
    "                'noisy_gating': noisy_gating,\n",
    "                'alpha_info': alpha_info,\n",
    "                'gating_layer': gating_layer}\n",
    "\n",
    "\n",
    "with open(os.path.join(output_dir, \"hyperparameters.json\"), \"w\") as f:\n",
    "    json.dump(loss_history, f)\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_gating_network\"), exist_ok=True)\n",
    "model.save_gating_network(os.path.join(output_dir, f\"trained_gating_network/{task_name_str}\"), task_name_str)\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_adapters\"), exist_ok=True)\n",
    "for adapter in loaded_adapters:\n",
    "    model.save_adapter(os.path.join(output_dir, f\"trained_adapters/{adapter}\"), adapter)\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_head\"), exist_ok=True)\n",
    "model.save_head(os.path.join(output_dir, f\"trained_head/{task_name_str}\"), task_name_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aed3544-4d5a-4f76-b82f-ddbbb680d485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[12.3778, 6.6641, 17.4005, 16.6528, 10.3441, 15.9994, 29.4254, 13.8707, 14.5707, 7.8677, 23.373, 11.8183, 15.4928, 21.9837, 28.3972, 9.7616]\" of type <class 'list'> for key \"eval/gate_freq_avg\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[[0.1088, 25.8902, 84.9737, 0.0009, 6.666, 0.6764, 84.575, 0.0019, 11.5685, 0.8743, 0.3602, 2.2655, 10.1585, 0.6914, 0.0159, 27.1726], [0.6801, 0.2064, 7.0835, 20.4784, 14.3161, 0.0216, 21.9456, 26.56, 33.0441, 0.1313, 2.6548, 1.6961, 1.728, 93.6501, 28.0525, 3.7514], [37.9737, 0.6764, 0.591, 8.3255, 0.9615, 0.8386, 0.3555, 0.0816, 15.0882, 0.0403, 88.03, 1.3096, 68.0816, 11.6407, 8.6051, 13.4006], [0.0188, 0.1642, 2.9428, 1.6895, 1.3039, 58.7495, 3.606, 15.0816, 0.0, 6.4362, 57.0882, 11.1332, 0.0, 40.5094, 56.4756, 0.8011], [0.1932, 0.0, 20.6238, 90.1323, 2.2552, 34.0957, 1.0084, 35.7786, 2.4878, 1.258, 0.0206, 23.1576, 4.2908, 6.2195, 2.4146, 32.0638], [8.1238, 23.3049, 0.0, 30.6857, 12.8086, 0.7833, 55.9259, 0.0, 4.1276, 14.3011, 0.3124, 49.3171, 0.3546, 3.5291, 50.3687, 2.0572], [12.5694, 0.1051, 55.2608, 11.5338, 5.1895, 3.4062, 2.7955, 0.4343, 3.2598, 2.969, 59.0638, 0.0976, 3.2054, 10.0122, 86.0488, 0.0488], [0.0, 9.6585, 25.061, 0.5816, 33.6567, 42.4953, 41.5188, 0.0, 0.0009, 42.8471, 45.3396, 4.6745, 0.0, 8.2129, 0.1811, 1.772], [32.9522, 16.7523, 0.2017, 5.5066, 15.651, 0.0919, 0.0009, 40.1482, 4.6811, 8.4869, 0.0, 0.5816, 35.8583, 0.5816, 61.4381, 33.0675], [41.833, 2.0, 0.0, 8.651, 6.4568, 7.2458, 73.2899, 3.0413, 1.0929, 14.8818, 2.136, 5.8593, 47.8734, 0.0, 41.5291, 0.1098], [2.1417, 0.167, 11.2505, 0.0919, 1.8161, 39.1482, 44.9587, 4.0056, 35.3415, 0.0009, 10.2205, 39.2608, 12.1426, 48.6079, 4.4503, 2.3959], [11.939, 1.0441, 0.8171, 22.1567, 23.0478, 4.44, 23.1248, 41.3152, 64.1557, 2.1857, 15.2505, 2.4672, 2.2205, 40.1501, 1.1867, 0.4991]]\" of type <class 'list'> for key \"eval/gate_freq_all\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8.0,\n",
      " 'eval_accuracy': 0.8902438879013062,\n",
      " 'eval_gate_freq_all': [[0.1088,\n",
      "                         25.8902,\n",
      "                         84.9737,\n",
      "                         0.0009,\n",
      "                         6.666,\n",
      "                         0.6764,\n",
      "                         84.575,\n",
      "                         0.0019,\n",
      "                         11.5685,\n",
      "                         0.8743,\n",
      "                         0.3602,\n",
      "                         2.2655,\n",
      "                         10.1585,\n",
      "                         0.6914,\n",
      "                         0.0159,\n",
      "                         27.1726],\n",
      "                        [0.6801,\n",
      "                         0.2064,\n",
      "                         7.0835,\n",
      "                         20.4784,\n",
      "                         14.3161,\n",
      "                         0.0216,\n",
      "                         21.9456,\n",
      "                         26.56,\n",
      "                         33.0441,\n",
      "                         0.1313,\n",
      "                         2.6548,\n",
      "                         1.6961,\n",
      "                         1.728,\n",
      "                         93.6501,\n",
      "                         28.0525,\n",
      "                         3.7514],\n",
      "                        [37.9737,\n",
      "                         0.6764,\n",
      "                         0.591,\n",
      "                         8.3255,\n",
      "                         0.9615,\n",
      "                         0.8386,\n",
      "                         0.3555,\n",
      "                         0.0816,\n",
      "                         15.0882,\n",
      "                         0.0403,\n",
      "                         88.03,\n",
      "                         1.3096,\n",
      "                         68.0816,\n",
      "                         11.6407,\n",
      "                         8.6051,\n",
      "                         13.4006],\n",
      "                        [0.0188,\n",
      "                         0.1642,\n",
      "                         2.9428,\n",
      "                         1.6895,\n",
      "                         1.3039,\n",
      "                         58.7495,\n",
      "                         3.606,\n",
      "                         15.0816,\n",
      "                         0.0,\n",
      "                         6.4362,\n",
      "                         57.0882,\n",
      "                         11.1332,\n",
      "                         0.0,\n",
      "                         40.5094,\n",
      "                         56.4756,\n",
      "                         0.8011],\n",
      "                        [0.1932,\n",
      "                         0.0,\n",
      "                         20.6238,\n",
      "                         90.1323,\n",
      "                         2.2552,\n",
      "                         34.0957,\n",
      "                         1.0084,\n",
      "                         35.7786,\n",
      "                         2.4878,\n",
      "                         1.258,\n",
      "                         0.0206,\n",
      "                         23.1576,\n",
      "                         4.2908,\n",
      "                         6.2195,\n",
      "                         2.4146,\n",
      "                         32.0638],\n",
      "                        [8.1238,\n",
      "                         23.3049,\n",
      "                         0.0,\n",
      "                         30.6857,\n",
      "                         12.8086,\n",
      "                         0.7833,\n",
      "                         55.9259,\n",
      "                         0.0,\n",
      "                         4.1276,\n",
      "                         14.3011,\n",
      "                         0.3124,\n",
      "                         49.3171,\n",
      "                         0.3546,\n",
      "                         3.5291,\n",
      "                         50.3687,\n",
      "                         2.0572],\n",
      "                        [12.5694,\n",
      "                         0.1051,\n",
      "                         55.2608,\n",
      "                         11.5338,\n",
      "                         5.1895,\n",
      "                         3.4062,\n",
      "                         2.7955,\n",
      "                         0.4343,\n",
      "                         3.2598,\n",
      "                         2.969,\n",
      "                         59.0638,\n",
      "                         0.0976,\n",
      "                         3.2054,\n",
      "                         10.0122,\n",
      "                         86.0488,\n",
      "                         0.0488],\n",
      "                        [0.0,\n",
      "                         9.6585,\n",
      "                         25.061,\n",
      "                         0.5816,\n",
      "                         33.6567,\n",
      "                         42.4953,\n",
      "                         41.5188,\n",
      "                         0.0,\n",
      "                         0.0009,\n",
      "                         42.8471,\n",
      "                         45.3396,\n",
      "                         4.6745,\n",
      "                         0.0,\n",
      "                         8.2129,\n",
      "                         0.1811,\n",
      "                         1.772],\n",
      "                        [32.9522,\n",
      "                         16.7523,\n",
      "                         0.2017,\n",
      "                         5.5066,\n",
      "                         15.651,\n",
      "                         0.0919,\n",
      "                         0.0009,\n",
      "                         40.1482,\n",
      "                         4.6811,\n",
      "                         8.4869,\n",
      "                         0.0,\n",
      "                         0.5816,\n",
      "                         35.8583,\n",
      "                         0.5816,\n",
      "                         61.4381,\n",
      "                         33.0675],\n",
      "                        [41.833,\n",
      "                         2.0,\n",
      "                         0.0,\n",
      "                         8.651,\n",
      "                         6.4568,\n",
      "                         7.2458,\n",
      "                         73.2899,\n",
      "                         3.0413,\n",
      "                         1.0929,\n",
      "                         14.8818,\n",
      "                         2.136,\n",
      "                         5.8593,\n",
      "                         47.8734,\n",
      "                         0.0,\n",
      "                         41.5291,\n",
      "                         0.1098],\n",
      "                        [2.1417,\n",
      "                         0.167,\n",
      "                         11.2505,\n",
      "                         0.0919,\n",
      "                         1.8161,\n",
      "                         39.1482,\n",
      "                         44.9587,\n",
      "                         4.0056,\n",
      "                         35.3415,\n",
      "                         0.0009,\n",
      "                         10.2205,\n",
      "                         39.2608,\n",
      "                         12.1426,\n",
      "                         48.6079,\n",
      "                         4.4503,\n",
      "                         2.3959],\n",
      "                        [11.939,\n",
      "                         1.0441,\n",
      "                         0.8171,\n",
      "                         22.1567,\n",
      "                         23.0478,\n",
      "                         4.44,\n",
      "                         23.1248,\n",
      "                         41.3152,\n",
      "                         64.1557,\n",
      "                         2.1857,\n",
      "                         15.2505,\n",
      "                         2.4672,\n",
      "                         2.2205,\n",
      "                         40.1501,\n",
      "                         1.1867,\n",
      "                         0.4991]],\n",
      " 'eval_gate_freq_avg': [12.3778,\n",
      "                        6.6641,\n",
      "                        17.4005,\n",
      "                        16.6528,\n",
      "                        10.3441,\n",
      "                        15.9994,\n",
      "                        29.4254,\n",
      "                        13.8707,\n",
      "                        14.5707,\n",
      "                        7.8677,\n",
      "                        23.373,\n",
      "                        11.8183,\n",
      "                        15.4928,\n",
      "                        21.9837,\n",
      "                        28.3972,\n",
      "                        9.7616],\n",
      " 'eval_loss': 0.20264207820097604,\n",
      " 'eval_loss_cls': 0.2930699785550435,\n",
      " 'eval_loss_gate': 0.11221416542927425,\n",
      " 'eval_runtime': 67.1034,\n",
      " 'eval_samples_per_second': 15.886,\n",
      " 'eval_steps_per_second': 0.045}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "pprint(metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53427fb5-595a-49be-8bce-688267001416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input('Remove files?\\n')\n",
    "# import shutil\n",
    "# directory_path = output_dir\n",
    "# shutil.rmtree(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c72d4361-cec6-4280-ba9f-1c772ffeadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "324f08ba-7e92-4600-aee4-0ec29ca08b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.roberta.encoder.layer:\n",
    "#     layer.output.gating_data.pop('gate_score')\n",
    "#     layer.output.gating_data.pop('gate_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d2045-45e8-494d-8705-c06b4ed33ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "pytorch2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
